{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Fraud Credit Card Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading credit card CSV file\n",
    "cc_df = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying first 5 rows of data\n",
    "cc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying data info\n",
    "cc_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing data\n",
    "cc_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the number of occurrences for each class. Non fraud = 1, fraud = 0\n",
    "non_fraud = len(cc_df[cc_df['Class'] == 0])\n",
    "fraud = len(cc_df[cc_df['Class'] == 1])\n",
    "fraud_percentage = (fraud/len(cc_df['Class'])) * 100\n",
    "\n",
    "print(f\"Number of genuine transactions: {non_fraud}\")\n",
    "print(f\"Number of fraud transactions: {fraud}\")\n",
    "print(f\"Percentage of fraud transactions: {fraud_percentage:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting heat map to see if any values are null\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(cc_df.isnull(), yticklabels=False, cbar=False, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot of genuine vs fraud transactions\n",
    "plt.title('Genuine vs Fraud Transactions')\n",
    "sns.countplot(x='Class', data=cc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of 'amount' column\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(x='Amount', data=cc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE Histogram of 'time' column\n",
    "sns.displot(x='Time', data=cc_df, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix of data\n",
    "corr = cc_df.corr()\n",
    "plt.figure(figsize=(30, 40))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling amount column data\n",
    "scaler = StandardScaler()\n",
    "cc_df['NormalizedAmount'] = scaler.fit_transform(cc_df['Amount'].values.reshape(-1, 1))\n",
    "\n",
    "# Dropping unnecessary columns\n",
    "cc_df.drop(['Amount', 'Time'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating data\n",
    "X = cc_df.drop('Class', axis=1)\n",
    "y = cc_df['Class']\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model\n",
    "rfc = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model predictions\n",
    "predictions = rfc.predict(X_test)\n",
    "# Accuracy score\n",
    "rfc_score = rfc.score(X_test, y_test) * 100\n",
    "rfc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method displays the performance metrics of the model \n",
    "def display_metrics(y_test, predictions):\n",
    "    print(\"Accuracy: {:.5f}\".format(accuracy_score(y_test, predictions)))\n",
    "    print(\"Precision: {:.5f}\".format(precision_score(y_test, predictions)))\n",
    "    print(\"Recall: {:.5f}\".format(recall_score(y_test, predictions)))\n",
    "    print(\"F1-score: {:.5f}\".format(f1_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying confusion matrix\n",
    "cf_matrix = confusion_matrix(y_test, predictions)\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "sns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt=\".0f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing metrics\n",
    "print('Random Forest Metrics')\n",
    "display_metrics(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling data because it is highly imbalanced.\n",
    "X_resampled, y_resampled = SMOTE().fit_resample(X, y)\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot of genuine vs fraud transactions\n",
    "plt.title('Genuine vs Fraud Transactions')\n",
    "sns.countplot(x=y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model\n",
    "rfc = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model predictions\n",
    "predictions = rfc.predict(X_test)\n",
    "# Accuracy score\n",
    "rfc_score = rfc.score(X_test, y_test) * 100\n",
    "rfc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying confusion matrix after oversampling data\n",
    "cf_matrix = confusion_matrix(y_test, predictions)\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "sns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt=\".0f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying metrics\n",
    "display_metrics(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method handles the user input and user interface portion\n",
    "def display_ui():\n",
    "    print(\"After training and testing the model, the program will now accurately predict a transaction as genuine or fraudulent\\n\")\n",
    "    print(\"Enter 'g' to randomly select a genuine transaction from the list of transactions. The model will then predict the class\")\n",
    "    print(\"Enter 'f' to randomly select a fraudulent transaction from the list of transactions. The model will then predict the class\")\n",
    "    print(\"Enter 'r' to randomly select any type of transaction for the model to predict. The model will then predict the class\")\n",
    "    print(\"\\n0 is a genuine transaction | 1 is a fraudulent transaction\\n\")\n",
    "    user_input = input(\"Enter Option - \").lower()\n",
    "    rand_transaction = None\n",
    "    if user_input == 'g': # genuine transaction\n",
    "        rand_transaction = (cc_df[cc_df['Class'] == 0].sample()).drop('Class', axis=1)\n",
    "        print(f\"******Transaction Data******\\n{rand_transaction.iloc[0]}\")\n",
    "        print(\"****************************\")\n",
    "        print(f\">Predicted Value - {rfc.predict(rand_transaction)} 'genuine' | Actual Value - [0] 'genuine'\")\n",
    "    elif user_input == 'f': # fraud transaction\n",
    "        rand_transaction = cc_df[cc_df['Class'] == 1].sample().drop('Class', axis=1)\n",
    "        print(f\"******Transaction Data******\\n{rand_transaction.iloc[0]}\")\n",
    "        print(\"****************************\")\n",
    "        print(f\">Predicted Value - {rfc.predict(rand_transaction)} 'fraudulent' | Actual Value - [1] 'fraudulent'\")\n",
    "    elif user_input == 'r': # random transaction\n",
    "        rand_transaction = (cc_df[(cc_df['Class'] == 1) | (cc_df['Class'] == 0)].sample())\n",
    "        print(f\"******Transaction Data******\\n{rand_transaction.drop('Class', axis=1).iloc[0]}\")\n",
    "        print(\"****************************\")\n",
    "        actual_val = math.floor(rand_transaction.iloc[0][28])\n",
    "        if actual_val == 0:\n",
    "            print(f\">Predicted Value - {rfc.predict(rand_transaction.drop('Class', axis=1))} 'genuine' | Actual Value - [{actual_val}] 'genuine'\")\n",
    "        else:\n",
    "            print(f\">Predicted Value - {rfc.predict(rand_transaction.drop('Class', axis=1))} 'fraudulent' | Actual Value - [{actual_val}] 'fraudulent'\")\n",
    "    else: # User entered an incorrect option\n",
    "        print('Please run cell again an enter a correct option')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying UI method\n",
    "display_ui()\n",
    "# RUN THIS CELL AGAIN FOR THE MODEL TO MAKE ANOTHER PREDICTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
